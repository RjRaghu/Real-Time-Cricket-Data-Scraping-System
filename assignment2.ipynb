{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Live Data: [{'live': True, 'name': ['BRH', 'SYS'], 'over': ['20.0', '8.4'], 'scores': ['138/9', '72/2'], 'link': 'https://crex.live/scoreboard/QCU/1MV/15th-Match/4J/4O/brh-vs-sys-15th-match-big-bash-league-2024-25/live'}]\n",
      "Upcoming Data: [{'time_start': '6:45 PM', 'type': '40th T10, Emirates D10 2024-25', 'name': ['Fujairah', 'Emirates Blues'], 'link': 'https://crex.live/scoreboard/SLM/1PZ/40th-Match/DD/DH/emb-vs-fuj-40th-match-emirates-d10-league-2024-25/info'}, {'time_start': '9:00 PM', 'type': '41st T10, Emirates D10 2024-25', 'name': ['Ajman', 'Sharjah'], 'link': 'https://crex.live/scoreboard/SLN/1PZ/41st-Match/DF/DG/ajm-vs-sha-41st-match-emirates-d10-league-2024-25/info'}, {'time_start': '9:30 PM', 'type': '6th T10, Barbados T10 2024-25', 'name': ['Warriors BRB', 'Pelicans'], 'link': 'https://crex.live/scoreboard/SN3/1Q1/6th-Match/10S/10T/pel-vs-war-6th-match-barbados-t10-2024-25/info'}, {'time_start': '11:15 PM', 'type': '42nd T10, Emirates D10 2024-25', 'name': ['Emirates Blues', 'Dubai'], 'link': 'https://crex.live/scoreboard/SLO/1PZ/42nd-Match/DD/DE/dub-vs-emb-42nd-match-emirates-d10-league-2024-25/info'}, {'time_start': '12:00 AM', 'type': '7th T10, Barbados T10 2024-25', 'name': ['Settlers', 'Guardians'], 'link': 'https://crex.live/scoreboard/SN4/1Q1/7th-Match/10P/10U/gua-vs-set-7th-match-barbados-t10-2024-25/info'}, {'time_start': '11:45 AM', 'type': '2nd T20, SL vs NZ 2024-25', 'name': ['New Zealand', 'Sri Lanka'], 'link': 'https://crex.live/scoreboard/QEI/1MZ/2nd-T20/R/T/nz-vs-sl-2nd-t20-sri-lanka-tour-of-new-zealand-2024-25/info'}, {'time_start': '1:00 PM', 'type': '1st T20, BPL 2024-25', 'name': ['Fortune Barishal', 'Durbar Rajshahi'], 'link': 'https://crex.live/scoreboard/S1N/1OV/1st-Match/AH/G8/dr-vs-frb-1st-match-bangladesh-premier-league-2024-25/info'}, {'time_start': '1:45 PM', 'type': '16th T20, BBL 2024-25', 'name': ['Sydney Thunder', 'Melbourne Renegades'], 'link': 'https://crex.live/scoreboard/QCV/1MV/16th-Match/4L/51/mlr-vs-syt-16th-match-big-bash-league-2024-25/info'}, {'time_start': '4:30 PM', 'type': '1st Semi Final T10, Emirates D10 2024-25', 'name': ['Team 1 (TBC)', 'Team 2 (TBC)'], 'link': 'https://crex.live/scoreboard/SLP/1PZ/1st-Semi-Final/CA/N/tbc-vs-tbc-1st-semi-final-emirates-d10-league-2024-25/info'}, {'time_start': '6:00 PM', 'type': '2nd T20, BPL 2024-25', 'name': ['Rangpur Riders', 'Dhaka Capitals'], 'link': 'https://crex.live/scoreboard/S1O/1OV/2nd-Match/5T/UC/dc-vs-rgr-2nd-match-bangladesh-premier-league-2024-25/info'}, {'time_start': '6:45 PM', 'type': '2nd Semi Final T10, Emirates D10 2024-25', 'name': ['Team 1 (TBC)', 'Team 2 (TBC)'], 'link': 'https://crex.live/scoreboard/SLQ/1PZ/2nd-Semi-Final/CA/N/tbc-vs-tbc-2nd-semi-final-emirates-d10-league-2024-25/info'}, {'time_start': '9:30 PM', 'type': '8th T10, Barbados T10 2024-25', 'name': ['Titans BRB', 'Warriors BRB'], 'link': 'https://crex.live/scoreboard/SN5/1Q1/8th-Match/10R/10T/tit-vs-war-8th-match-barbados-t10-2024-25/info'}, {'time_start': '10:45 PM', 'type': 'Final T10, Emirates D10 2024-25', 'name': ['Team 1 (TBC)', 'Team 2 (TBC)'], 'link': 'https://crex.live/scoreboard/SLR/1PZ/Final/CA/N/tbc-vs-tbc-final-emirates-d10-league-2024-25/info'}, {'time_start': '12:00 AM', 'type': '9th T10, Barbados T10 2024-25', 'name': ['Voyagers', 'Pelicans'], 'link': 'https://crex.live/scoreboard/SN6/1Q1/9th-Match/10Q/10S/pel-vs-voy-9th-match-barbados-t10-2024-25/info'}, {'time_start': '5:10 AM', 'type': '4th T20, W-Super Smash 2024-25', 'name': ['Otago Women', 'Central Women'], 'link': 'https://crex.live/scoreboard/SB6/1PT/4th-Match/JT/JX/chw-vs-osw-4th-match-womens-super-smash-2024-25/info'}, {'time_start': '8:55 AM', 'type': '4th T20, Super Smash 2024-25', 'name': ['Otago', 'Central Stags'], 'link': 'https://crex.live/scoreboard/REZ/1OK/4th-Match/52/56/cs-vs-otg-4th-match-super-smash-2024-25/info'}, {'time_start': '9:00 AM', 'type': '73rd ODI, VHT 2024-25', 'name': ['Punjab', 'Saurashtra'], 'link': 'https://crex.live/scoreboard/SF9/1M1/73rd-Match/GT/HC/pun-vs-saur-73rd-match-vijay-hazare-trophy-2024-25/info'}, {'time_start': '9:00 AM', 'type': '74th ODI, VHT 2024-25', 'name': ['Hyderabad', 'Karnataka'], 'link': 'https://crex.live/scoreboard/SFA/1M1/74th-Match/H5/H9/hyd-vs-kar-74th-match-vijay-hazare-trophy-2024-25/info'}, {'time_start': '9:00 AM', 'type': '75th ODI, VHT 2024-25', 'name': ['Tamil Nadu', 'Vidarbha'], 'link': 'https://crex.live/scoreboard/SFB/1M1/75th-Match/GU/H7/tn-vs-vid-75th-match-vijay-hazare-trophy-2024-25/info'}]\n",
      "\n",
      "--- Match Info ---\n",
      "Element not found within the timeout period\n",
      "{}\n",
      "\n",
      "--- Scorecard ---\n"
     ]
    },
    {
     "ename": "TimeoutException",
     "evalue": "Message: \nStacktrace:\n\tGetHandleVerifier [0x00007FF79BCAFB05+28789]\n\t(No symbol) [0x00007FF79BC186E0]\n\t(No symbol) [0x00007FF79BAB592A]\n\t(No symbol) [0x00007FF79BB0930E]\n\t(No symbol) [0x00007FF79BB095FC]\n\t(No symbol) [0x00007FF79BB528A7]\n\t(No symbol) [0x00007FF79BB2F47F]\n\t(No symbol) [0x00007FF79BB4F654]\n\t(No symbol) [0x00007FF79BB2F1E3]\n\t(No symbol) [0x00007FF79BAFA938]\n\t(No symbol) [0x00007FF79BAFBAA1]\n\tGetHandleVerifier [0x00007FF79BFE933D+3410093]\n\tGetHandleVerifier [0x00007FF79BFFE7DD+3497293]\n\tGetHandleVerifier [0x00007FF79BFF2A73+3448803]\n\tGetHandleVerifier [0x00007FF79BD77BBB+848171]\n\t(No symbol) [0x00007FF79BC23C3F]\n\t(No symbol) [0x00007FF79BC1F6E4]\n\t(No symbol) [0x00007FF79BC1F87D]\n\t(No symbol) [0x00007FF79BC0ED49]\n\tBaseThreadInitThunk [0x00007FF99142E8D7+23]\n\tRtlUserThreadStart [0x00007FF99265FBCC+44]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 483\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Scorecard ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    482\u001b[0m scorecard_url \u001b[38;5;241m=\u001b[39m base_match_url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/scorecard\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 483\u001b[0m sc_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_scorecard_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscorecard_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28mprint\u001b[39m(sc_data)\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# Live => /live (usually only relevant if match is currently live)\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# But we can still attempt to scrape it\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 292\u001b[0m, in \u001b[0;36mget_scorecard_data\u001b[1;34m(scorecard_url)\u001b[0m\n\u001b[0;32m    288\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(scorecard_url)  \u001b[38;5;66;03m# e.g. \"https://crex.live/<match-id>/scorecard\"\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;66;03m# Wait for a known element that indicates the scorecard is loaded\u001b[39;00m\n\u001b[1;32m--> 292\u001b[0m     \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLASS_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscorecard-wrapper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(driver\u001b[38;5;241m.\u001b[39mpage_source, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;66;03m# 1) Venue & Date\u001b[39;00m\n",
      "File \u001b[1;32md:\\Cricradio\\env\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:105\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll)\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[1;31mTimeoutException\u001b[0m: Message: \nStacktrace:\n\tGetHandleVerifier [0x00007FF79BCAFB05+28789]\n\t(No symbol) [0x00007FF79BC186E0]\n\t(No symbol) [0x00007FF79BAB592A]\n\t(No symbol) [0x00007FF79BB0930E]\n\t(No symbol) [0x00007FF79BB095FC]\n\t(No symbol) [0x00007FF79BB528A7]\n\t(No symbol) [0x00007FF79BB2F47F]\n\t(No symbol) [0x00007FF79BB4F654]\n\t(No symbol) [0x00007FF79BB2F1E3]\n\t(No symbol) [0x00007FF79BAFA938]\n\t(No symbol) [0x00007FF79BAFBAA1]\n\tGetHandleVerifier [0x00007FF79BFE933D+3410093]\n\tGetHandleVerifier [0x00007FF79BFFE7DD+3497293]\n\tGetHandleVerifier [0x00007FF79BFF2A73+3448803]\n\tGetHandleVerifier [0x00007FF79BD77BBB+848171]\n\t(No symbol) [0x00007FF79BC23C3F]\n\t(No symbol) [0x00007FF79BC1F6E4]\n\t(No symbol) [0x00007FF79BC1F87D]\n\t(No symbol) [0x00007FF79BC0ED49]\n\tBaseThreadInitThunk [0x00007FF99142E8D7+23]\n\tRtlUserThreadStart [0x00007FF99265FBCC+44]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "# Optional scheduling\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1) SCRAPE MAIN FIXTURE LIST (live vs. upcoming)\n",
    "# ------------------------------------------------------------------------------\n",
    "def get_match_data():\n",
    "    \"\"\"\n",
    "    Scrapes the main fixture list page (https://crex.live/fixtures/match-list).\n",
    "    Returns two lists:\n",
    "      - live_data: Info about currently live matches\n",
    "      - upcoming_data: Info about future matches\n",
    "    \"\"\"\n",
    "\n",
    "    webdriver_path = \"chromedriver.exe\"\n",
    "    service = Service(webdriver_path)\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    url = \"https://crex.live/fixtures/match-list\"\n",
    "    driver.get(url)\n",
    "\n",
    "    live_data = []\n",
    "    upcoming_data = []\n",
    "\n",
    "    try:\n",
    "        # Wait until the match cards are present\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"match-card-container\"))\n",
    "        )\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        matches = soup.find_all(class_=\"match-card-container\")\n",
    "\n",
    "        for match in matches:\n",
    "            # Check if match is LIVE\n",
    "            if match.find(class_=\"liveTag\"):\n",
    "                link_tag = match.find(\"a\", href=True)\n",
    "                href = \"https://crex.live\" + link_tag[\"href\"] if link_tag else \"\"\n",
    "\n",
    "                teams_div = match.find_all(\"div\", class_=\"team-info\")\n",
    "                team_name = []\n",
    "                team_overs = []\n",
    "                team_scores = []\n",
    "\n",
    "                for t_div in teams_div:\n",
    "                    name_el = t_div.find(class_=\"team-name\")\n",
    "                    over_el = t_div.find(class_=\"total-overs\")\n",
    "                    score_el = t_div.find(class_=\"team-score\")\n",
    "\n",
    "                    name = name_el.text.strip() if name_el else \"N/A\"\n",
    "                    over = over_el.text.strip() if over_el else \"Yet to bat\"\n",
    "                    score = score_el.text.strip() if score_el else \"N/A\"\n",
    "\n",
    "                    team_name.append(name)\n",
    "                    team_overs.append(over)\n",
    "                    team_scores.append(score)\n",
    "\n",
    "                live_data.append(\n",
    "                    {\n",
    "                        \"live\": True,\n",
    "                        \"name\": team_name,\n",
    "                        \"over\": team_overs,\n",
    "                        \"scores\": team_scores,\n",
    "                        \"link\": href,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Check if match is UPCOMING\n",
    "            elif match.find(class_=\"not-started\"):\n",
    "                link_tag = match.find(\"a\", href=True)\n",
    "                href = \"https://crex.live\" + link_tag[\"href\"] if link_tag else \"\"\n",
    "\n",
    "                time_start_el = match.find(class_=\"start-text\")\n",
    "                match_type_el = match.find(class_=\"time\")\n",
    "\n",
    "                time_start = time_start_el.text.strip() if time_start_el else \"N/A\"\n",
    "                match_type = match_type_el.text.strip() if match_type_el else \"N/A\"\n",
    "\n",
    "                teams_div = match.find_all(\"div\", class_=\"team-info\")\n",
    "                team_name = [\n",
    "                    (\n",
    "                        t_div.find(class_=\"team-name\").text.strip()\n",
    "                        if t_div.find(class_=\"team-name\")\n",
    "                        else \"N/A\"\n",
    "                    )\n",
    "                    for t_div in teams_div\n",
    "                ]\n",
    "\n",
    "                upcoming_data.append(\n",
    "                    {\n",
    "                        \"time_start\": time_start,\n",
    "                        \"type\": match_type,\n",
    "                        \"name\": team_name,\n",
    "                        \"link\": href,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return live_data, upcoming_data\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2) SCRAPE “MATCH INFO” TAB => /info\n",
    "# ------------------------------------------------------------------------------\n",
    "def scrape_match_info(info_url):\n",
    "    \"\"\"\n",
    "    Scrapes data from the \"Match Info\" tab at (match_url + \"/info\").\n",
    "    Typically includes toss, venue, series, date, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    webdriver_path = \"chromedriver.exe\"\n",
    "    service = Service(webdriver_path)\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.get(info_url)  # e.g., \"https://crex.live/<match-id>/info\"\n",
    "\n",
    "    try:\n",
    "        # Wait for the match info container\n",
    "        try:\n",
    "            WebDriverWait(driver, 30).until(  # Increase timeout to 30 seconds\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"match-info-card\"))\n",
    "            )\n",
    "        except TimeoutException:\n",
    "            print(\"Element not found within the timeout period\")\n",
    "            return {} \n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # 1) Venue & Date\n",
    "        match_venue_el = soup.find(class_=\"match-date match-venue\")\n",
    "        match_venue = match_venue_el.text.strip() if match_venue_el else \"N/A\"\n",
    "\n",
    "        match_date_el = soup.find(class_=\"match-info-date\") or soup.find(\n",
    "            class_=\"match-date\"\n",
    "        )\n",
    "        match_date = match_date_el.text.strip() if match_date_el else \"N/A\"\n",
    "\n",
    "        # 2) Teams\n",
    "        teams_name = []\n",
    "        teams_el = soup.find_all(class_=\"form-team-name\")\n",
    "        for team_el in teams_el:\n",
    "            teams_name.append(team_el.get_text(strip=True) if team_el else \"N/A\")\n",
    "\n",
    "        # 3) Series Name\n",
    "        series_name_el = soup.find(class_=\"s-name\")\n",
    "        series_name = series_name_el.text.strip() if series_name_el else \"N/A\"\n",
    "\n",
    "        # 4) Toss info\n",
    "        toss_el = soup.find(class_=\"toss-wrap\")\n",
    "        if toss_el:\n",
    "            toss_p = toss_el.find(\"p\")\n",
    "            toss_info = toss_p.get_text(strip=True) if toss_p else \"N/A\"\n",
    "        else:\n",
    "            toss_info = \"N/A\"\n",
    "\n",
    "        # 5) Head-to-head\n",
    "        head_to_head = []\n",
    "        team1_wins_el = soup.find(class_=\"team1-wins\")\n",
    "        team2_wins_el = soup.find(class_=\"team2-wins\")\n",
    "        head_to_head.append(team1_wins_el.text if team1_wins_el else \"N/A\")\n",
    "        head_to_head.append(team2_wins_el.text if team2_wins_el else \"N/A\")\n",
    "\n",
    "        # 6) Match result or highlight\n",
    "        match_result = []\n",
    "        matches = soup.find_all(class_=\"global-match-card gmc-without-logo\")\n",
    "        for m in matches:\n",
    "            match_result.append(m.text.strip() if m else \"N/A\")\n",
    "\n",
    "        # 7) Additional weather/venue stats\n",
    "        table_el = soup.find(class_=\"table table-borderless colHeader\")\n",
    "        table = table_el.text.strip() if table_el else \"N/A\"\n",
    "\n",
    "        venue_details_el = soup.find(class_=\"align-center weather-wrap\")\n",
    "        venue_details = venue_details_el.text.strip() if venue_details_el else \"N/A\"\n",
    "\n",
    "        venue_stats_el = soup.find(class_=\"venue-left-wrapper\")\n",
    "        venue_stats = venue_stats_el.text.strip() if venue_stats_el else \"N/A\"\n",
    "\n",
    "        pace_vs_spin_on_venue_el = soup.find(class_=\"venue-pace-wrap\")\n",
    "        pace_vs_spin_on_venue = (\n",
    "            pace_vs_spin_on_venue_el.text.strip() if pace_vs_spin_on_venue_el else \"N/A\"\n",
    "        )\n",
    "\n",
    "        # Compile\n",
    "        match_info_data = {\n",
    "            \"match_venue\": match_venue,\n",
    "            \"match_date\": match_date,\n",
    "            \"teams_name\": teams_name,\n",
    "            \"series_name\": series_name,\n",
    "            \"toss_info\": toss_info,\n",
    "            \"head_to_head\": head_to_head,\n",
    "            \"match_result\": match_result,\n",
    "            \"scorecard_table\": table,\n",
    "            \"venue_details\": venue_details,\n",
    "            \"venue_stats\": venue_stats,\n",
    "            \"pace_vs_spin_on_venue\": pace_vs_spin_on_venue,\n",
    "        }\n",
    "\n",
    "        return match_info_data\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3) SCRAPE “LIVE” TAB => /live\n",
    "# ------------------------------------------------------------------------------\n",
    "def scrape_live_data(live_url):\n",
    "    \"\"\"\n",
    "    Scrapes data from the \"Live\" tab at (match_url + \"/live\").\n",
    "    Useful when the match is actually in progress; might contain\n",
    "    ball-by-ball updates, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    webdriver_path = \"chromedriver.exe\"\n",
    "    service = Service(webdriver_path)\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.get(live_url)  # e.g., \"https://crex.live/<match-id>/live\"\n",
    "\n",
    "    try:\n",
    "        # Wait for some element that indicates live data has loaded\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"live-wrapper\"))\n",
    "        )\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Example: Extract some live commentary or data\n",
    "        # If you have a container with class .live-wrapper that holds ball-by-ball:\n",
    "        live_container = soup.find(class_=\"live-wrapper\")\n",
    "        live_data = live_container.get_text(strip=True) if live_container else \"N/A\"\n",
    "\n",
    "        # You might have sub-sections like .ball-details, .commentary, etc.\n",
    "        # Adjust accordingly\n",
    "\n",
    "        return {\"live_data\": live_data}\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4) SCRAPE “SCORECARD” TAB => /scorecard\n",
    "# ------------------------------------------------------------------------------\n",
    "def get_scorecard_data(scorecard_url):\n",
    "    \"\"\"\n",
    "    Scrapes details from the \"Scorecard\" tab (match_url + \"/scorecard\").\n",
    "    For a live or upcoming match, it might show partial or different data.\n",
    "    \"\"\"\n",
    "\n",
    "    webdriver_path = \"chromedriver.exe\"\n",
    "    service = Service(webdriver_path)\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.get(scorecard_url)  # e.g. \"https://crex.live/<match-id>/scorecard\"\n",
    "\n",
    "    try:\n",
    "        # Wait for a known element that indicates the scorecard is loaded\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"scorecard-wrapper\"))\n",
    "        )\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # 1) Venue & Date\n",
    "        match_venue_el = soup.find(class_=\"match-date match-venue\")\n",
    "        match_venue = match_venue_el.text.strip() if match_venue_el else \"N/A\"\n",
    "\n",
    "        match_date_el = soup.find(class_=\"match-info-date\") or soup.find(\n",
    "            class_=\"match-date\"\n",
    "        )\n",
    "        match_date = match_date_el.text.strip() if match_date_el else \"N/A\"\n",
    "\n",
    "        # 2) Series Name\n",
    "        series_name_el = soup.find(class_=\"s-name\")\n",
    "        series_name = series_name_el.text.strip() if series_name_el else \"N/A\"\n",
    "\n",
    "        # 3) Toss info (if present)\n",
    "        toss_el = soup.find(class_=\"toss-wrap\")\n",
    "        if toss_el:\n",
    "            toss_p = toss_el.find(\"p\")\n",
    "            toss_info = toss_p.get_text(strip=True) if toss_p else \"N/A\"\n",
    "        else:\n",
    "            toss_info = \"N/A\"\n",
    "\n",
    "        # 4) Head-to-head or other data\n",
    "        head_to_head = []\n",
    "        team1_wins_el = soup.find(class_=\"team1-wins\")\n",
    "        team2_wins_el = soup.find(class_=\"team2-wins\")\n",
    "        head_to_head.append(team1_wins_el.text if team1_wins_el else \"N/A\")\n",
    "        head_to_head.append(team2_wins_el.text if team2_wins_el else \"N/A\")\n",
    "\n",
    "        # 5) Match results\n",
    "        match_result = []\n",
    "        matches = soup.find_all(class_=\"global-match-card gmc-without-logo\")\n",
    "        for m in matches:\n",
    "            match_result.append(m.text.strip() if m else \"N/A\")\n",
    "\n",
    "        # 6) Scorecard table\n",
    "        table_el = soup.find(class_=\"table table-borderless colHeader\")\n",
    "        table = table_el.text.strip() if table_el else \"N/A\"\n",
    "\n",
    "        # 7) Venue details\n",
    "        venue_details_el = soup.find(class_=\"align-center weather-wrap\")\n",
    "        venue_details = venue_details_el.text.strip() if venue_details_el else \"N/A\"\n",
    "\n",
    "        venue_stats_el = soup.find(class_=\"venue-left-wrapper\")\n",
    "        venue_stats = venue_stats_el.text.strip() if venue_stats_el else \"N/A\"\n",
    "\n",
    "        pace_vs_spin_on_venue_el = soup.find(class_=\"venue-pace-wrap\")\n",
    "        pace_vs_spin_on_venue = (\n",
    "            pace_vs_spin_on_venue_el.text.strip() if pace_vs_spin_on_venue_el else \"N/A\"\n",
    "        )\n",
    "\n",
    "        scorecard_data = {\n",
    "            \"match_venue\": match_venue,\n",
    "            \"match_date\": match_date,\n",
    "            \"series_name\": series_name,\n",
    "            \"toss_info\": toss_info,\n",
    "            \"head_to_head\": head_to_head,\n",
    "            \"match_result\": match_result,\n",
    "            \"scorecard_table\": table,\n",
    "            \"venue_details\": venue_details,\n",
    "            \"venue_stats\": venue_stats,\n",
    "            \"pace_vs_spin_on_venue\": pace_vs_spin_on_venue,\n",
    "        }\n",
    "\n",
    "        return scorecard_data\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "# def scrape_squads_with_clicks(match_url):\n",
    "#     webdriver_path = \"chromedriver.exe\"\n",
    "#     service = Service(webdriver_path)\n",
    "#     options = Options()\n",
    "#     options.add_argument(\"--headless\")\n",
    "#     options.add_argument(\"--disable-gpu\")\n",
    "#     options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "#     driver = webdriver.Chrome(service=service, options=options)\n",
    "#     squads_url = match_url\n",
    "#     driver.get(squads_url)\n",
    "\n",
    "#     data = {}\n",
    "\n",
    "#     try:\n",
    "#         # Wait for the team buttons to show up\n",
    "#         WebDriverWait(driver, 10).until(\n",
    "#             EC.presence_of_all_elements_located((By.CLASS_NAME, \"playingxi-button\"))\n",
    "#         )\n",
    "\n",
    "#         # Collect all team buttons\n",
    "#         team_buttons = driver.find_elements(By.CLASS_NAME, \"playingxi-button\")\n",
    "\n",
    "#         all_teams = []\n",
    "\n",
    "#         for btn in team_buttons:\n",
    "#             # Get the team name from the button text\n",
    "#             team_name = btn.text.strip()\n",
    "\n",
    "#             # Click the button to reveal that team's squad\n",
    "#             btn.click()\n",
    "\n",
    "#             # Wait for the playing XI to show\n",
    "#             WebDriverWait(driver, 10).until(\n",
    "#                 EC.presence_of_element_located((By.CLASS_NAME, \"info-right-wrapper\"))\n",
    "#             )\n",
    "\n",
    "#             # Parse the updated DOM\n",
    "#             page_content = driver.page_source\n",
    "#             soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "\n",
    "#             # Now find playingxi-card / on-bench-wrap for this team\n",
    "#             playing_div = soup.find(\"div\", class_=\"playingxi-card\")\n",
    "#             bench_div = soup.find(\"div\", class_=\"playingxi-card on-bench-wrap\")\n",
    "\n",
    "#             # ----- Extract Playing XI -----\n",
    "#             playing_players = []\n",
    "#             if playing_div:\n",
    "#                 # Each player row might look like:\n",
    "#                 # <div class=\"player-row\">\n",
    "#                 #     <div class=\"p-name\">Player Name</div>\n",
    "#                 #     <div class=\"bat-ball-type\">All-rounder</div>\n",
    "#                 # </div>\n",
    "#                 player_rows = playing_div.find_all(\"div\", class_=\"player-row\")\n",
    "#                 for row in player_rows:\n",
    "#                     name_div = row.find(\"div\", class_=\"p-name\")\n",
    "#                     type_div = row.find(\"div\", class_=\"bat-ball-type\")\n",
    "\n",
    "#                     player_name = name_div.get_text(strip=True) if name_div else \"N/A\"\n",
    "#                     player_type = type_div.get_text(strip=True) if type_div else \"N/A\"\n",
    "\n",
    "#                     playing_players.append(\n",
    "#                         {\"player_name\": player_name, \"player_type\": player_type}\n",
    "#                     )\n",
    "\n",
    "#             # ----- Extract Bench Players -----\n",
    "#             bench_players = []\n",
    "#             if bench_div:\n",
    "#                 bench_rows = bench_div.find_all(\"div\", class_=\"player-row\")\n",
    "#                 for row in bench_rows:\n",
    "#                     name_div = row.find(\"div\", class_=\"p-name\")\n",
    "#                     type_div = row.find(\"div\", class_=\"bat-ball-type\")\n",
    "\n",
    "#                     player_name = name_div.get_text(strip=True) if name_div else \"N/A\"\n",
    "#                     player_type = type_div.get_text(strip=True) if type_div else \"N/A\"\n",
    "\n",
    "#                     bench_players.append(\n",
    "#                         {\"player_name\": player_name, \"player_type\": player_type}\n",
    "#                     )\n",
    "\n",
    "#             # Store the results for each team\n",
    "#             all_teams.append(\n",
    "#                 {\n",
    "#                     \"team_name\": team_name,\n",
    "#                     \"playing_11\": playing_players,\n",
    "#                     \"on_bench\": bench_players,\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#         data[\"squads\"] = all_teams if all_teams else \"N/A\"\n",
    "\n",
    "#     finally:\n",
    "#         driver.quit()\n",
    "\n",
    "#     return data\n",
    "\n",
    "\n",
    "\n",
    "live_data, upcoming_data = get_match_data()\n",
    "print(\"Live Data:\", live_data)\n",
    "print(\"Upcoming Data:\", upcoming_data)\n",
    "\n",
    "# 2) If you want to pick a match (live or upcoming) and fetch data:\n",
    "# For demonstration, pick the first upcoming match\n",
    "if upcoming_data:\n",
    "    base_match_url = upcoming_data[0][\"link\"]\n",
    "\n",
    "    # Match Info => /info\n",
    "    print(\"\\n--- Match Info ---\")\n",
    "    info_url = base_match_url + \"/info\"\n",
    "    info_data = scrape_match_info(info_url)\n",
    "    print(info_data)\n",
    "\n",
    "    # Scorecard => /scorecard\n",
    "    print(\"\\n--- Scorecard ---\")\n",
    "    scorecard_url = base_match_url + \"/scorecard\"\n",
    "    sc_data = get_scorecard_data(scorecard_url)\n",
    "    print(sc_data)\n",
    "\n",
    "    # Live => /live (usually only relevant if match is currently live)\n",
    "    # But we can still attempt to scrape it\n",
    "    print(\"\\n--- Live Tab ---\")\n",
    "    live_tab_url = base_match_url + \"/live\"\n",
    "    live_tab_data = scrape_live_data(live_tab_url)\n",
    "    print(live_tab_data)\n",
    "\n",
    "# 3) If there's a live match, do similarly:\n",
    "if live_data:\n",
    "    base_match_url = live_data[0][\"link\"]\n",
    "\n",
    "    print(\"\\n--- Live Match Info (/info) ---\")\n",
    "    info_data = scrape_match_info(base_match_url + \"/info\")\n",
    "    print(info_data)\n",
    "\n",
    "    print(\"\\n--- Live Match Scorecard (/scorecard) ---\")\n",
    "    sc_data = get_scorecard_data(base_match_url + \"/scorecard\")\n",
    "    print(sc_data)\n",
    "\n",
    "    print(\"\\n--- Live Match Live Tab (/live) ---\")\n",
    "    live_tab_data = scrape_live_data(base_match_url + \"/live\")\n",
    "    print(live_tab_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import streamlit as st\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1) SCRAPE MAIN FIXTURE LIST (live vs. upcoming)\n",
    "# ----------------------------------------------------------------------\n",
    "def get_match_data():\n",
    "    \"\"\"\n",
    "    Scrapes the main fixture list page (https://crex.live/fixtures/match-list).\n",
    "    Returns two lists:\n",
    "      - live_data: Info about currently live matches\n",
    "      - upcoming_data: Info about future matches\n",
    "    \"\"\"\n",
    "    webdriver_path = \"chromedriver.exe\"\n",
    "    service = Service(webdriver_path)\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    url = \"https://crex.live/fixtures/match-list\"\n",
    "    driver.get(url)\n",
    "\n",
    "    live_data = []\n",
    "    upcoming_data = []\n",
    "\n",
    "    try:\n",
    "        # === Make Selenium WAIT up to 10 seconds for the match cards ===\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"match-card-container\"))\n",
    "        )\n",
    "\n",
    "        # Now the page (or at least the match-card-container) is loaded\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        matches = soup.find_all(class_=\"match-card-container\")\n",
    "\n",
    "        for match in matches:\n",
    "            # Check if match is LIVE\n",
    "            if match.find(class_=\"liveTag\"):\n",
    "                link_tag = match.find(\"a\", href=True)\n",
    "                href = \"https://crex.live\" + link_tag[\"href\"] if link_tag else \"\"\n",
    "\n",
    "                teams_div = match.find_all(\"div\", class_=\"team-info\")\n",
    "                team_name = []\n",
    "                team_overs = []\n",
    "                team_scores = []\n",
    "\n",
    "                for t_div in teams_div:\n",
    "                    name_el = t_div.find(class_=\"team-name\")\n",
    "                    over_el = t_div.find(class_=\"total-overs\")\n",
    "                    score_el = t_div.find(class_=\"team-score\")\n",
    "\n",
    "                    name = name_el.text.strip() if name_el else \"N/A\"\n",
    "                    over = over_el.text.strip() if over_el else \"Yet to bat\"\n",
    "                    score = score_el.text.strip() if score_el else \"N/A\"\n",
    "\n",
    "                    team_name.append(name)\n",
    "                    team_overs.append(over)\n",
    "                    team_scores.append(score)\n",
    "\n",
    "                live_data.append(\n",
    "                    {\n",
    "                        \"live\": True,\n",
    "                        \"name\": team_name,\n",
    "                        \"over\": team_overs,\n",
    "                        \"scores\": team_scores,\n",
    "                        \"link\": href,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Otherwise, check if match is UPCOMING\n",
    "            elif match.find(class_=\"not-started\"):\n",
    "                link_tag = match.find(\"a\", href=True)\n",
    "                href = \"https://crex.live\" + link_tag[\"href\"] if link_tag else \"\"\n",
    "\n",
    "                time_start_el = match.find(class_=\"start-text\")\n",
    "                match_type_el = match.find(class_=\"time\")\n",
    "\n",
    "                time_start = time_start_el.text.strip() if time_start_el else \"N/A\"\n",
    "                match_type = match_type_el.text.strip() if match_type_el else \"N/A\"\n",
    "\n",
    "                teams_div = match.find_all(\"div\", class_=\"team-info\")\n",
    "                team_name = [\n",
    "                    (\n",
    "                        t_div.find(class_=\"team-name\").text.strip()\n",
    "                        if t_div.find(class_=\"team-name\")\n",
    "                        else \"N/A\"\n",
    "                    )\n",
    "                    for t_div in teams_div\n",
    "                ]\n",
    "\n",
    "                upcoming_data.append(\n",
    "                    {\n",
    "                        \"time_start\": time_start,\n",
    "                        \"type\": match_type,\n",
    "                        \"name\": team_name,\n",
    "                        \"link\": href,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return live_data, upcoming_data\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2) SCRAPE “MATCH INFO” TAB => /info\n",
    "# ----------------------------------------------------------------------\n",
    "def scrape_match_info(info_url):\n",
    "    \"\"\"\n",
    "    Scrapes data from the \"Match Info\" tab at (match_url + \"/info\").\n",
    "    Typically includes toss, venue, series, date, etc.\n",
    "    \"\"\"\n",
    "    webdriver_path = \"chromedriver.exe\"\n",
    "    service = Service(webdriver_path)\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.get(info_url)\n",
    "\n",
    "    try:\n",
    "        # Increase timeout to allow slow loads\n",
    "\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"match-info-card\"))\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        match_venue_el = soup.find(class_=\"match-date match-venue\")\n",
    "        match_venue = match_venue_el.text.strip() if match_venue_el else \"N/A\"\n",
    "\n",
    "        match_date_el = soup.find(class_=\"match-info-date\") or soup.find(\n",
    "            \"div\", class_=\"match-date\"\n",
    "        )\n",
    "        match_date = match_date_el.text.strip() if match_date_el else \"N/A\"\n",
    "\n",
    "        teams_name = []\n",
    "        teams_el = soup.find_all(class_=\"form-team-name\")\n",
    "        for team_el in teams_el:\n",
    "            teams_name.append(team_el.get_text(strip=True) if team_el else \"N/A\")\n",
    "\n",
    "        series_name_el = soup.find(class_=\"s-name\")\n",
    "        series_name = series_name_el.text.strip() if series_name_el else \"N/A\"\n",
    "\n",
    "        toss_el = soup.find(class_=\"toss-wrap\")\n",
    "        if toss_el:\n",
    "            toss_p = toss_el.find(\"p\")\n",
    "            toss_info = toss_p.get_text(strip=True) if toss_p else \"N/A\"\n",
    "        else:\n",
    "            toss_info = \"N/A\"\n",
    "\n",
    "        head_to_head = []\n",
    "        team1_wins_el = soup.find(class_=\"team1-wins\")\n",
    "        team2_wins_el = soup.find(class_=\"team2-wins\")\n",
    "        head_to_head.append(team1_wins_el.text if team1_wins_el else \"N/A\")\n",
    "        head_to_head.append(team2_wins_el.text if team2_wins_el else \"N/A\")\n",
    "\n",
    "        match_result = []\n",
    "        matches = soup.find_all(class_=\"global-match-card gmc-without-logo\")\n",
    "        for m in matches:\n",
    "            match_result.append(m.text.strip() if m else \"N/A\")\n",
    "\n",
    "        table_el = soup.find(class_=\"table table-borderless colHeader\")\n",
    "        table = table_el.text.strip() if table_el else \"N/A\"\n",
    "\n",
    "        venue_details_el = soup.find(class_=\"align-center weather-wrap\")\n",
    "        venue_details = venue_details_el.text.strip() if venue_details_el else \"N/A\"\n",
    "\n",
    "        venue_stats_el = soup.find(class_=\"venue-left-wrapper\")\n",
    "        venue_stats = venue_stats_el.text.strip() if venue_stats_el else \"N/A\"\n",
    "\n",
    "        pace_vs_spin_on_venue_el = soup.find(class_=\"venue-pace-wrap\")\n",
    "        pace_vs_spin_on_venue = (\n",
    "            pace_vs_spin_on_venue_el.text.strip() if pace_vs_spin_on_venue_el else \"N/A\"\n",
    "        )\n",
    "\n",
    "        match_info_data = {\n",
    "            \"match_venue\": match_venue,\n",
    "            \"match_date\": match_date,\n",
    "            \"teams_name\": teams_name,\n",
    "            \"series_name\": series_name,\n",
    "            \"toss_info\": toss_info,\n",
    "            \"head_to_head\": head_to_head,\n",
    "            \"match_result\": match_result,\n",
    "            \"scorecard_table\": table,\n",
    "            \"venue_details\": venue_details,\n",
    "            \"venue_stats\": venue_stats,\n",
    "            \"pace_vs_spin_on_venue\": pace_vs_spin_on_venue,\n",
    "        }\n",
    "        return match_info_data\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3) SCRAPE “LIVE” TAB => /live\n",
    "# ----------------------------------------------------------------------\n",
    "def scrape_live_data(live_url, match_info=None):\n",
    "    \"\"\"\n",
    "    Scrapes data from the \"Live\" tab at (match_url + \"/live\").\n",
    "    Extracts team names and scores, including runs and overs.\n",
    "    If Team 2's name is missing or invalid, it falls back to match_info.\n",
    "    \"\"\"\n",
    "    webdriver_path = \"chromedriver.exe\"\n",
    "    service = Service(webdriver_path)\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.get(live_url)\n",
    "\n",
    "    try:\n",
    "        # Wait for the live-score-card to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"live-score-card\"))\n",
    "        )\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Find the live-score-card container\n",
    "        live_container = soup.find(class_=\"live-score-card\")\n",
    "        if not live_container:\n",
    "            return {\"live_data\": \"N/A\"}\n",
    "\n",
    "        # Initialize data structure\n",
    "        live_data = {}\n",
    "\n",
    "        # Extract data for both teams\n",
    "        team_divs = live_container.find_all(class_=\"team-content\")\n",
    "        for index, team_div in enumerate(team_divs):\n",
    "            team_name_el = team_div.find(class_=\"team-name\")\n",
    "            score_div = team_div.find(class_=\"team-score\")\n",
    "\n",
    "            # Extract runs and overs from spans\n",
    "            if score_div:\n",
    "                spans = score_div.find_all(\"span\")\n",
    "                runs = spans[0].text.strip() if len(spans) > 0 else \"N/A\"\n",
    "                overs = spans[1].text.strip() if len(spans) > 1 else \"N/A\"\n",
    "            else:\n",
    "                runs = \"N/A\"\n",
    "                overs = \"N/A\"\n",
    "\n",
    "            # Extract team name\n",
    "            team_name = team_name_el.text.strip() if team_name_el else \"N/A\"\n",
    "\n",
    "            # Validate Team 2's name\n",
    "            if index == 1 and (not team_name or team_name.lower() == \"team\"):\n",
    "                # Fallback to match_info if provided\n",
    "                if (\n",
    "                    match_info\n",
    "                    and \"teams_name\" in match_info\n",
    "                    and len(match_info[\"teams_name\"]) > 1\n",
    "                ):\n",
    "                    team_name = match_info[\"teams_name\"][1]\n",
    "\n",
    "            # Team data\n",
    "            live_data[f\"team_{index + 1}\"] = {\n",
    "                \"name\": team_name,\n",
    "                \"runs\": runs,\n",
    "                \"overs\": overs,\n",
    "            }\n",
    "\n",
    "        return live_data\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(\"Timeout: live-score-card not found on live page.\")\n",
    "        return {\"live_data\": \"N/A\"}\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4) SCRAPE “SCORECARD” TAB => /scorecard\n",
    "# ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# 4) SCRAPE “SCORECARD” TAB => /scorecard\n",
    "# ----------------------------------------------------------------------\n",
    "def scrape_partnerships(soup):\n",
    "    \"\"\"\n",
    "    Extract the Partnerships section from the scorecard page.\n",
    "    Args:\n",
    "        soup (BeautifulSoup): The parsed HTML of the scorecard page.\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing partnership data.\n",
    "    \"\"\"\n",
    "    partnerships_data = []\n",
    "\n",
    "    # Locate the partnerships section by class\n",
    "    partnership_section = soup.find(\"div\", class_=\"partnership-section\")\n",
    "    if not partnership_section:\n",
    "        print(\"Partnerships section not found.\")\n",
    "        return partnerships_data\n",
    "\n",
    "    # Parse each partnership block\n",
    "    partnership_blocks = partnership_section.find_all(\"div\", class_=\"p-section-wrapper\")\n",
    "    for block in partnership_blocks:\n",
    "        # Extract wicket information\n",
    "        wicket_info = block.find(\"div\", class_=\"p-wckt-info\")\n",
    "        wicket = wicket_info.text.strip() if wicket_info else \"N/A\"\n",
    "\n",
    "        # Extract partnership details\n",
    "        partnership_info = block.find(\"div\", class_=\"p-info-wrapper\")\n",
    "        if not partnership_info:\n",
    "            continue\n",
    "\n",
    "        data_points = partnership_info.find_all(\"div\", class_=\"p-data\")\n",
    "        if len(data_points) >= 3:\n",
    "            batter1 = data_points[0].find(\"p\").text.strip()\n",
    "            batter1_stats = (\n",
    "                data_points[0].find(\"span\", class_=\"run-highlight\").text.strip()\n",
    "            )\n",
    "\n",
    "            total_runs = data_points[1].find(\"p\", class_=\"p-runs\").text.strip()\n",
    "\n",
    "            batter2 = data_points[2].find(\"p\").text.strip()\n",
    "            batter2_stats = (\n",
    "                data_points[2].find(\"span\", class_=\"run-highlight\").text.strip()\n",
    "            )\n",
    "\n",
    "            partnerships_data.append(\n",
    "                {\n",
    "                    \"wicket\": wicket,\n",
    "                    \"batter1\": batter1,\n",
    "                    \"batter1_stats\": batter1_stats,\n",
    "                    \"total_runs\": total_runs,\n",
    "                    \"batter2\": batter2,\n",
    "                    \"batter2_stats\": batter2_stats,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return partnerships_data\n",
    "\n",
    "\n",
    "def get_scorecard_data(scorecard_url):\n",
    "    \"\"\"\n",
    "    Scrapes details from the \"Scorecard\" tab (match_url + \"/scorecard\").\n",
    "    Extracts batting, bowling, fall of wickets, and partnerships sections dynamically.\n",
    "    \"\"\"\n",
    "    webdriver_path = \"chromedriver.exe\"\n",
    "    service = Service(webdriver_path)\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.get(scorecard_url)\n",
    "\n",
    "    try:\n",
    "        # Wait for the scorecard page to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"score\"))\n",
    "        )\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        scorecard_data = {\n",
    "            \"batting\": [],\n",
    "            \"bowling\": [],\n",
    "            \"fall_of_wickets\": [],\n",
    "            \"partnerships\": [],\n",
    "        }\n",
    "\n",
    "        # Scrap batting and bowling sections\n",
    "        table_headings = soup.find_all(\"div\", class_=\"table-heading\")\n",
    "        for table_heading in table_headings:\n",
    "            heading_text = table_heading.find(\"h3\").text.strip()\n",
    "\n",
    "            # Find the next sibling div containing the score-card\n",
    "            score_card = table_heading.find_next_sibling(\n",
    "                \"div\", class_=\"card score-card\"\n",
    "            )\n",
    "            if not score_card:\n",
    "                continue  # Skip if no score-card found\n",
    "\n",
    "            score_table = score_card.find(\"table\", class_=\"bowler-table\")\n",
    "            if not score_table:\n",
    "                continue  # Skip if no table found\n",
    "\n",
    "            rows = score_table.find(\"tbody\").find_all(\"tr\")\n",
    "            section_data = []\n",
    "            if heading_text.lower() == \"batting\":\n",
    "                for row in rows:\n",
    "                    cells = row.find_all(\"td\")\n",
    "                    if len(cells) >= 6:\n",
    "                        section_data.append(\n",
    "                            {\n",
    "                                \"batter\": cells[0]\n",
    "                                .find(\"span\", class_=\"player-name\")\n",
    "                                .text.strip(),\n",
    "                                \"runs\": cells[1].text.strip(),\n",
    "                                \"balls\": cells[2].text.strip(),\n",
    "                                \"fours\": cells[3].text.strip(),\n",
    "                                \"sixes\": cells[4].text.strip(),\n",
    "                                \"strike_rate\": cells[5].text.strip(),\n",
    "                            }\n",
    "                        )\n",
    "                scorecard_data[\"batting\"].append(section_data)\n",
    "\n",
    "            elif heading_text.lower() == \"bowling\":\n",
    "                for row in rows:\n",
    "                    cells = row.find_all(\"td\")\n",
    "                    if len(cells) >= 6:\n",
    "                        section_data.append(\n",
    "                            {\n",
    "                                \"bowler\": cells[0]\n",
    "                                .find(\"span\", class_=\"player-name\")\n",
    "                                .text.strip(),\n",
    "                                \"overs\": cells[1].text.strip(),\n",
    "                                \"maidens\": cells[2].text.strip(),\n",
    "                                \"runs_conceded\": cells[3].text.strip(),\n",
    "                                \"wickets\": cells[4].text.strip(),\n",
    "                                \"economy\": cells[5].text.strip(),\n",
    "                            }\n",
    "                        )\n",
    "                scorecard_data[\"bowling\"].append(section_data)\n",
    "\n",
    "        # Scrap Fall of Wickets\n",
    "        scorecard_data[\"fall_of_wickets\"] = scrape_fall_of_wickets(soup)\n",
    "\n",
    "        # Scrap Partnerships\n",
    "        scorecard_data[\"partnerships\"] = scrape_partnerships(soup)\n",
    "\n",
    "        return scorecard_data\n",
    "\n",
    "    except TimeoutException:\n",
    "        # print(\"Timeout: Unable to load scorecard.\")\n",
    "        return {\"Match not started Yet\"}\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "def scrape_fall_of_wickets(soup):\n",
    "    \"\"\"\n",
    "    Extract the Fall of Wickets section from the scorecard page.\n",
    "    Args:\n",
    "        soup (BeautifulSoup): The parsed HTML of the scorecard page.\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing fall of wickets data.\n",
    "    \"\"\"\n",
    "    fall_of_wickets_data = []\n",
    "\n",
    "    # Locate the \"Fall of Wickets\" section by heading\n",
    "    fall_of_wickets_heading = soup.find(\"h3\", text=\"FALL OF WICKETS\")\n",
    "    if not fall_of_wickets_heading:\n",
    "        print(\"Fall of Wickets section not found.\")\n",
    "        return fall_of_wickets_data\n",
    "\n",
    "    # Find the parent container of the \"Fall of Wickets\" table\n",
    "    fall_of_wickets_section = fall_of_wickets_heading.find_next(\n",
    "        \"div\", class_=\"card score-card\"\n",
    "    )\n",
    "    if not fall_of_wickets_section:\n",
    "        print(\"Fall of Wickets table not found.\")\n",
    "        return fall_of_wickets_data\n",
    "\n",
    "    # Locate the table inside the section\n",
    "    fall_of_wickets_table = fall_of_wickets_section.find(\"table\", class_=\"bowler-table\")\n",
    "    if not fall_of_wickets_table:\n",
    "        print(\"Fall of Wickets table structure not found.\")\n",
    "        return fall_of_wickets_data\n",
    "\n",
    "    # Parse the table rows\n",
    "    rows = fall_of_wickets_table.find(\"tbody\").find_all(\"tr\")\n",
    "    for row in rows:\n",
    "        cells = row.find_all(\"td\")\n",
    "        if len(cells) >= 3:\n",
    "            batsman = cells[0].find(\"span\", class_=\"player-name\")\n",
    "            batsman_name = batsman.text.strip() if batsman else \"N/A\"\n",
    "            score = cells[1].text.strip() if cells[1] else \"N/A\"\n",
    "            overs = cells[2].text.strip() if cells[2] else \"N/A\"\n",
    "\n",
    "            fall_of_wickets_data.append(\n",
    "                {\n",
    "                    \"batsman\": batsman_name,\n",
    "                    \"score\": score,\n",
    "                    \"overs\": overs,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return fall_of_wickets_data\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 5) SCRAPE “SQUADS” => /squads (with button clicks)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def scrape_squads_with_clicks(match_url):\n",
    "    \"\"\"\n",
    "    Scrapes the squads via /squads. Within .info_right_wrapper there are:\n",
    "      - Buttons (class 'playingxi-button') for each team.\n",
    "      - 'playingxi-card' containers for playing XI\n",
    "      - 'playingxi-card on-bench-wrap' containers for bench\n",
    "      - Rows with class 'playingxi-card-row', each containing .p-name and .bat-ball-type\n",
    "    \"\"\"\n",
    "\n",
    "    webdriver_path = \"chromedriver.exe\"\n",
    "    service = Service(webdriver_path)\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    squads_url = match_url  # E.g., https://crex.live/match-id/squads\n",
    "    driver.get(squads_url)\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Wait for .info-right-wrapper\n",
    "        WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"info-right-wrapper\"))\n",
    "        )\n",
    "\n",
    "        # If even .info-right-wrapper never appears, just fallback\n",
    "\n",
    "        # 2) Wait or check for the team buttons\n",
    "\n",
    "        WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, \"playingxi-button\"))\n",
    "        )\n",
    "\n",
    "        # If even .info-right-wrapper never appears, just fallback\n",
    "\n",
    "        # 3) Grab all 'playingxi-button' elements\n",
    "        try:\n",
    "            team_buttons = driver.find_elements(By.CLASS_NAME, \"playingxi-button\")\n",
    "        except TimeoutException:\n",
    "            return {\"squads\": \"N/A\"}\n",
    "        all_teams = []\n",
    "\n",
    "        # 4) For each button (Team A, Team B, etc.)\n",
    "        for btn in team_buttons:\n",
    "            team_name = btn.text.strip()\n",
    "\n",
    "            # 4a) Use JavaScript click to avoid potential overlay issues\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].click();\", btn)\n",
    "            except ElementClickInterceptedException:\n",
    "                # If still intercepted, we can attempt scroll-into-view or handle an ad\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView(true);\", btn)\n",
    "                time.sleep(1)\n",
    "                driver.execute_script(\"arguments[0].click();\", btn)\n",
    "\n",
    "            # 4b) Grab updated DOM\n",
    "            page_content = driver.page_source\n",
    "            soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "\n",
    "            # 5) Identify containers for playing XI vs bench\n",
    "            playing_div = soup.find(\"div\", class_=\"playingxi-card\")\n",
    "            bench_div = soup.find(\"div\", class_=\"playingxi-card on-bench-wrap\")\n",
    "\n",
    "            playing_players = []\n",
    "            if playing_div:\n",
    "                # 5a) Each row is .playingxi-card-row\n",
    "                rows = playing_div.find_all(\"div\", class_=\"playingxi-card-row\")\n",
    "                for row in rows:\n",
    "                    name_div = row.find(\"div\", class_=\"p-name\")\n",
    "                    type_div = row.find(\"div\", class_=\"bat-ball-type\")\n",
    "                    player_name = name_div.get_text(strip=True) if name_div else \"N/A\"\n",
    "                    player_type = type_div.get_text(strip=True) if type_div else \"N/A\"\n",
    "                    playing_players.append(\n",
    "                        {\"player_name\": player_name, \"player_type\": player_type}\n",
    "                    )\n",
    "\n",
    "            bench_players = []\n",
    "            if bench_div:\n",
    "                # 5b) Same logic for bench rows\n",
    "                rows = bench_div.find_all(\"div\", class_=\"playingxi-card-row\")\n",
    "                for row in rows:\n",
    "                    name_div = row.find(\"div\", class_=\"p-name\")\n",
    "                    type_div = row.find(\"div\", class_=\"bat-ball-type\")\n",
    "                    player_name = name_div.get_text(strip=True) if name_div else \"N/A\"\n",
    "                    player_type = type_div.get_text(strip=True) if type_div else \"N/A\"\n",
    "                    bench_players.append(\n",
    "                        {\"player_name\": player_name, \"player_type\": player_type}\n",
    "                    )\n",
    "\n",
    "            # 6) Append data for this team\n",
    "            all_teams.append(\n",
    "                {\n",
    "                    \"team_name\": team_name,\n",
    "                    \"playing_11\": playing_players,\n",
    "                    \"on_bench\": bench_players,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # 7) Final result\n",
    "        data[\"squads\"] = all_teams if all_teams else \"N/A\"\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutException",
     "evalue": "Message: \nStacktrace:\n\tGetHandleVerifier [0x00007FF756D3FB05+28789]\n\t(No symbol) [0x00007FF756CA86E0]\n\t(No symbol) [0x00007FF756B4592A]\n\t(No symbol) [0x00007FF756B9930E]\n\t(No symbol) [0x00007FF756B995FC]\n\t(No symbol) [0x00007FF756BE28A7]\n\t(No symbol) [0x00007FF756BBF47F]\n\t(No symbol) [0x00007FF756BDF654]\n\t(No symbol) [0x00007FF756BBF1E3]\n\t(No symbol) [0x00007FF756B8A938]\n\t(No symbol) [0x00007FF756B8BAA1]\n\tGetHandleVerifier [0x00007FF75707933D+3410093]\n\tGetHandleVerifier [0x00007FF75708E7DD+3497293]\n\tGetHandleVerifier [0x00007FF757082A73+3448803]\n\tGetHandleVerifier [0x00007FF756E07BBB+848171]\n\t(No symbol) [0x00007FF756CB3C3F]\n\t(No symbol) [0x00007FF756CAF6E4]\n\t(No symbol) [0x00007FF756CAF87D]\n\t(No symbol) [0x00007FF756C9ED49]\n\tBaseThreadInitThunk [0x00007FF99976E8D7+23]\n\tRtlUserThreadStart [0x00007FF99B25FBCC+44]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 148\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(result, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 148\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 142\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m    141\u001b[0m     match_link \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://crex.live/scoreboard/S1Q/1OV/4th-Match/5T/7G/rgr-vs-sys-4th-match-bangladesh-premier-league-2024-25/info\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 142\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_all_tabs_for_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatch_link\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(result, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "Cell \u001b[1;32mIn[7], line 129\u001b[0m, in \u001b[0;36mscrape_all_tabs_for_match\u001b[1;34m(match_link)\u001b[0m\n\u001b[0;32m    126\u001b[0m live_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/live\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    127\u001b[0m scorecard_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/scorecard\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 129\u001b[0m info_data \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_match_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m live_data \u001b[38;5;241m=\u001b[39m scrape_live_data(live_url)\n\u001b[0;32m    131\u001b[0m scorecard_data \u001b[38;5;241m=\u001b[39m get_scorecard_data(scorecard_url)\n",
      "Cell \u001b[1;32mIn[7], line 25\u001b[0m, in \u001b[0;36mscrape_match_info\u001b[1;34m(info_url)\u001b[0m\n\u001b[0;32m     22\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(info_url)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m     \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLASS_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatch-info-card\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(driver\u001b[38;5;241m.\u001b[39mpage_source, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m     match_venue \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch-date match-venue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mif\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind(class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch-date match-venue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32md:\\Cricradio\\env\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:105\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll)\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[1;31mTimeoutException\u001b[0m: Message: \nStacktrace:\n\tGetHandleVerifier [0x00007FF756D3FB05+28789]\n\t(No symbol) [0x00007FF756CA86E0]\n\t(No symbol) [0x00007FF756B4592A]\n\t(No symbol) [0x00007FF756B9930E]\n\t(No symbol) [0x00007FF756B995FC]\n\t(No symbol) [0x00007FF756BE28A7]\n\t(No symbol) [0x00007FF756BBF47F]\n\t(No symbol) [0x00007FF756BDF654]\n\t(No symbol) [0x00007FF756BBF1E3]\n\t(No symbol) [0x00007FF756B8A938]\n\t(No symbol) [0x00007FF756B8BAA1]\n\tGetHandleVerifier [0x00007FF75707933D+3410093]\n\tGetHandleVerifier [0x00007FF75708E7DD+3497293]\n\tGetHandleVerifier [0x00007FF757082A73+3448803]\n\tGetHandleVerifier [0x00007FF756E07BBB+848171]\n\t(No symbol) [0x00007FF756CB3C3F]\n\t(No symbol) [0x00007FF756CAF6E4]\n\t(No symbol) [0x00007FF756CAF87D]\n\t(No symbol) [0x00007FF756C9ED49]\n\tBaseThreadInitThunk [0x00007FF99976E8D7+23]\n\tRtlUserThreadStart [0x00007FF99B25FBCC+44]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
